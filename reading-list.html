<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Knowing Machines - Critical Dataset Studies Reading List</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Bootstrap icons-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel="stylesheet" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body class="d-flex flex-column">
        <main class="flex-shrink-0">
            <!-- Navigation-->
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container px-5">
                    <a class="navbar-brand" href="index.html">Knowing Machines</a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                      <ul class="navbar-nav ms-auto mb-2 mb-lg-0">
                        <li class="nav-item"><a class="nav-link text-white" href="index.html">Home</a></li>
                        <li class="nav-item"><a class="nav-link text-white" href="team.html">Team</a></li>
                        <li class="nav-item"><a class="nav-link text-white" href="reading-list.html">Critical Dataset Studies Reading List</a></li>

                        <li class="nav-item"><a class="nav-link text-white" href="contact.html">Contact</a></li>
                      </ul>
                    </div>
                </div>
            </nav>
            <!-- Page Content-->
            <section class="py-5">
                <div class="container px-5 my-5">
                    <div class="row gx-5">

                        <div class="col-lg-9">
                            <!-- Post content-->
                            <article>
                                <!-- Post header-->
                                <header class="mb-4">
                                    <!-- Post title-->
                                    <h1 class="fw-bolder mb-1">Critical Dataset Studies Reading List</h1>
                                    <!-- Post meta content-->
                                    <div class="text-muted fst-italic mb-2">Contributors: Frances Corry, Edward B. Kang, Hamsini Sridharan, Sasha Luccioni, Mike Ananny, Kate Crawford</div>
                                    <div class="text-muted fst-italic mb-2"> </div>
                                    <div class="text-muted fst-italic mb-2">Last updated: 22 September 2022</div>
                                    <!-- Post categories-->
                                    <!-- <a class="badge bg-secondary text-decoration-none link-light" href="#!">Web Design</a>
                                    <a class="badge bg-secondary text-decoration-none link-light" href="#!">Freebies</a> -->
                                </header>
                                <!-- Preview image figure-->
                                <!-- <figure class="mb-4"><img class="img-fluid rounded" src="assets/img/header-km1.jpg" alt="Image from Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence" /></figure> -->
                                <!-- Post content-->
                                <section class="mb-5">
                                  <p class="fs-5 mb-4">How should we study datasets in machine learning? As machine learning (ML) increasingly becomes a site of sociotechnical inquiry, invoking numerous social, political, legal, and ethical issues, datasets are a crucial component as they are core material used to train models. Inspired by Tarleton Gillespie and Nick Seaver&rsquo;s <a href="https://socialmediacollective.org/reading-lists/critical-algorithm-studies/" target="_blank" rel="noopener">Critical Algorithm Studies reading list</a>, this collection is meant to serve as an entry point to the growing literature on ML datasets across the fields of computer science, human-computer interaction, science and technology studies, media studies, and histories of technology, among others. We compiled this list primarily as a resource for researchers seeking to understand&mdash;from a variety of perspectives&mdash;how ML datasets work, do work, and are worked upon. We hope it will also be of use to technology practitioners and students seeking to build ML systems.</p>


<p class="fs-5 mb-4">We limit our scope to works that focus on datasets deployed in the training and testing of ML systems, and despite some overlap, this list is not a primer for the field of critical technology studies more generally. Entries
  are sorted into various sections with the intention of providing readers a preliminary structure that will help them follow their specific interests. We acknowledge that classificatory practice is always subjective and that many of these titles can
  fit appropriately under multiple sections or named in different ways. The current iteration is a reflection of our own ideas and what we find helpful as a way to organize the emerging literature that we are working with. There are certainly other
  ways to structure this reading list, and we are open to suggestions that expand its range and improve usability. Our focus is primarily on academic publications, but for those who are more interested in understanding how datasets have been
  discussed in the press as of July 2022, we offer a selection of examples at the end of the reading list.&nbsp;</p>
<p class="fs-5 mb-4">This list is also not meant to be exhaustive. We see the list as a living resource and invite readers to make suggestions and contributions via <a href="#submitform">this form</a>  if there
  are key titles that they think should be included. Please note that while all links are functional as of July 2022, we are unable to continuously monitor for updated versions of papers or fix broken links.</p>
<p class="fs-5 mb-4">Despite these limitations, we hope this reading list might serve as a useful resource for scholars and practitioners investigating ML datasets as sociotechnical assemblages that shape and are shaped by social worlds.</p>

<h1 class="fs-5 mb-4"><strong>Table of Contents</strong></h1>
<p class="fs-5 mb-4"><a href="#starting_points">1. STARTING POINTS</a></p>
<p class="fs-5 mb-4"><a href="#contextualizing_the_study_of_datasets">2. CONTEXTUALIZING THE STUDY OF DATASETS</a></p>
<p class="fs-5 mb-4"><a href="#public_sources_of_datasets">3. PUBLIC SOURCES OF DATASETS</a></p>
<p class="fs-5 mb-4"><a href="#studying_dataset_production">4. STUDYING DATASET PRODUCTION</a></p>
<p class="fs-5 mb-4"><a href="#analyses_of_training_datasets">5. ANALYSES OF TRAINING DATASETS</a></p>
<p class="fs-5 mb-4"><a href="#responses_to_dataset_problems">6. RESPONSES TO DATASET PROBLEMS</a></p>
<p class="fs-5 mb-4"><a href="#dataset_documentation_practices">7. DATASET DOCUMENTATION PRACTICES</a></p>
<p class="fs-5 mb-4"><a href="#conferences_focused_on_datasets">8. CONFERENCES FOCUSED ON DATASETS</a></p>
<p class="fs-5 mb-4"><a href="#press_treatment_of_datasets">9. PRESS TREATMENT OF DATASETS</a></p>



<h2 class="fs-5 mb-4" id="starting_points"><strong>1. STARTING POINTS&nbsp;</strong></h2>
<p class="fs-5 mb-4">This section contains a broad set of introductory texts and locales to ground the study of training data. Resources included in this section cover the politics, possibilities, and pitfalls of ML training data and offer early
  provocations for thinking about particular aspects of training data, such as privacy or bias.</p>
<ul>
  <li>Barocas, S., &amp; Selbst, A. D. (2016). Big Data&rsquo;s Disparate Impact. <em>California Law Review,</em><em>104</em>(3), 671&ndash;732. <a href="https://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf"
      target="_blank" rel="noopener">https://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf</a></li>
  <li>Crawford, K. (2021). <em>Atlas of AI: Power, Politics and the Planetary Costs of Artificial Intelligence, </em>see &lsquo;Data&rsquo; chapter (pp. 89-122). New Haven, CT: Yale University Press.&nbsp;</li>
  <li>Crawford, K., &amp; Paglen, T. (2019). Excavating AI: The Politics of Images in Machine Learning Training Sets. <a href="https://excavating.ai" target="_blank" rel="noopener">https://excavating.ai</a></li>
  <li>Denton, E., Hanna, A., Amironesei, R., Smart, A., Nicole, H., &amp; Scheuerman, M. K. (2020). Bringing the People Back In: Contesting Benchmark Machine Learning Datasets. 6. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2007.07399"
      target="_blank" rel="noopener">https://arxiv.org/abs/2007.07399</a></li>
  <li>Harvey, A. (2021). <em>Exposing.ai: Face and Biometric Image Datasets</em>. <a href="https://exposing.ai/datasets/" target="_blank" rel="noopener">https://exposing.ai/datasets/</a></li>
  <li>MacKenzie, A., &amp; Munster, A. (2019). Platform Seeing: Image Ensembles and Their Invisualities. <em>Theory, Culture &amp; Society, 36</em>(5), 3&ndash;22. <a href="https://doi.org/10.1177/0263276419847508" target="_blank"
      rel="noopener">https://doi.org/10.1177/0263276419847508</a></li>
  <li>Miceli, M., Posada, J., &amp; Yang, T. (2022). Studying Up Machine Learning Data: Why Talk About Bias When We Mean Power? <em>Proceedings of the ACM on Human-Computer Interaction, 6</em>(GROUP), 1&ndash;14. <a
      href="https://doi.org/10.1145/3492853" target="_blank" rel="noopener">https://doi.org/10.1145/3492853</a></li>
  <li><span style="color: #222222;">Paullada, A., Raji, I. D., Bender, E. M., Denton, E., &amp; Hanna, A. (2020). Data and Its (Dis)Contents: A Survey of Dataset Development and Use in Machine Learning Research. </span><span
      style="color: #222222;"><em>ArXiv</em></span><span style="color: #222222;">. </span><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2012.05345v1__;!!LIr3w8kk_Xxm!8b-dYz-Rsa7iMiBfoUZfFcOUl8g_my0VNHnOmE-6dmTJHkV49S2av1if6pi7lHgn$"
      target="_blank" rel="noopener">https://arxiv.org/abs/2012.05345v1</a></li>
  <li>Roberge, J., &amp; Castelle, M. (Eds.). (2020). <em>The Cultural Life of Machine Learning: An Incursion into Critical AI Studies</em> (1st ed. 2021 edition). Palgrave Macmillan.</li>
  <li>Srinivasan, R., &amp; Chander, A. (2021). Biases in AI Systems: A Survey for Practitioners. <em>Queue, 19</em>(2), 45-64. <a href="https://doi.org/10.1145/3466132.3466134" target="_blank"
      rel="noopener">https://doi.org/10.1145/3466132.3466134</a></li>
  <li>Suresh, H., &amp; Guttag, J. V. (2021). A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1901.10002" target="_blank"
      rel="noopener">http://arxiv.org/abs/1901.10002</a></li>
      <li>Thylstrup, N. B. (2022). The Ethics and Politics of Data Sets in the Age of Machine Learning: Deleting Traces and Encountering Remains. <em>Media, Culture & Society.</em> <a href="https://doi.org/10.1177/01634437211060226">https://doi.org/10.1177/01634437211060226</a></li>
</ul>
<h2 class="fs-5 mb-4" id="contextualizing_the_study_of_datasets"><strong>2. CONTEXTUALIZING THE STUDY OF DATASETS</strong></h2>
<p class="fs-5 mb-4">This section consists of broader foundational readings that don&rsquo;t all necessarily deal specifically with machine learning datasets, but which the authors of this list have found useful to contextualize their study. We
  acknowledge that the titles below do not form an exhaustive index of all foundational readings, but point to them as particularly helpful ones for thinking about the ontological and epistemological complexities of the &ldquo;dataset&rdquo; as an
  object/genre of analysis.&nbsp;</p>
<h3 class="fs-5 mb-4"><strong>a. Politics of Classification</strong></h3>
<p class="fs-5 mb-4">This subsection focuses on classification as a practice of not only world-ordering, but also world-making, and how its logics underlie the ways in which datasets are conceived and built.&nbsp;</p>
<ul>
  <li>Boutyline, A., &amp; Soter, L. K. Cultural Schemas: What They Are, How to Find Them, and What to Do Once You&rsquo;ve Caught One. <em>American Sociological Review, 86</em>(4), 728&ndash;758. <a href="https://doi.org/10.1177/00031224211024525"
      target="_blank" rel="noopener">https://doi.org/10.1177/00031224211024525</a></li>
  <li>Bechmann, A., &amp; Bowker, G. C. (2019). Unsupervised by any other name: Hidden layers of knowledge production in artificial intelligence on social media. <em>Big Data &amp; Society</em>, 6(1). <a href="https://doi.org/10.1177/2053951718819569" target="_blank" rel="noopener">https://doi.org/10.1177/2053951718819569</a></li>
  <li>Bowker, G. C., &amp; Star, S. L. (2000). <em>Sorting Things Out: Classification and Its Consequences</em>. Cambridge, MA: MIT Press.</li>
  <li>Crawford, K. (2021). <em>Atlas of AI: Power, Politics and the Planetary Costs of Artificial Intelligence</em>, see &lsquo;Classification&rsquo; chapter (pp. 123-150). New Haven, CT: Yale University Press.&nbsp;</li>
  <li>Fourcade, M., &amp; Healy, K. (2013). Classification Situations: Life-Chances in the Neoliberal Era. <em>Accounting, Organizations and Society, 38</em>(8), 559-572. <a href="https://doi.org/10.1016/j.aos.2013.11.002" target="_blank"
      rel="noopener">https://doi.org/10.1016/j.aos.2013.11.002</a>. </li>
  <li>Goodwin, C. (2000). Practices of Color Classification. <em>Mind, Culture, and Activity, 7</em>(1&amp;2), 19-36. <a href="https://doi.org/10.1080/10749039.2000.9677646" target="_blank"
      rel="noopener">https://doi.org/10.1080/10749039.2000.9677646</a></li>
  <li>Rieder, B. (2017). Scrutinizing an Algorithmic Technique: The Bayes Classifier as Interested Reading of Reality. <em>Information, Communication &amp; Society, 20</em>(1), 100-117. <a href="https://doi.org/10.1080/1369118X.2016.1181195"
      target="_blank" rel="noopener">https://doi.org/10.1080/1369118X.2016.1181195</a></li>
  <li>Sadre-Orafai, S. (2020). Typologies, Typifications, and Types. <em>Annual Review of Anthropology, 49</em>(1), 193-208. <a href="https://doi.org/10.1146/annurev-anthro-102218-011235" target="_blank"
      rel="noopener">https://doi.org/10.1146/annurev-anthro-102218-011235</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>b. Critical Data Studies&nbsp;</strong></h3>
<p class="fs-5 mb-4">Here, we introduce a few titles from the emerging field of Critical Data Studies which we believe are especially useful for the purposes of acquiring a nuanced and interdisciplinary understanding of datasets.&nbsp;</p>
<ul>
  <li>Andrejevic, M. (2019). <em>Automated Media</em> (1st edition). Routledge.</li>
  <li>Beer, D. (2018). <em>The Data Gaze</em>. London, UK: SAGE.</li>
  <li>Cheney-Lippold, J. (2017). <em>We Are Data: Algorithms and the Making of our Digital Selves</em>. New York, NY: NYU Press.</li>
  <li>Chun, W. (2021). <em>Discriminating Data</em>. Cambridge, MA: MIT Press.</li>
  <li>Cifor, M., Garcia, P., Cowan, T. L., Rault, J., Sutherland, T., Chan, A., . . . Nakamura, L. (2019). <em>Feminist Data Manifest-No</em>. Retrieved from <a href="https://www.manifestno.com/" target="_blank"
      rel="noopener">https://www.manifestno.com/</a></li>
  <li>Couldry, N., &amp; Mejias, U. A. (2019). <em>The Costs of Connection: How Data Is Colonizing Human Life and Appropriating It for Capitalism</em>. Stanford, CA: Stanford University Press.</li>
  <li>D&rsquo;Ignazio, C., &amp; Klein, L. F. (2020). <em>Data Feminism.</em> MIT Press.</li>
  <li>Gitelman L. (2013). <em>&ldquo;Raw Data&rdquo; Is an Oxymoron</em>. MIT Press.</li>
  <li>Hansson, K., &amp; Dahlgren, A. (2022). Open research data repositories: Practices, norms, and metadata for sharing images. <em> Journal of the Association for Information Science and Technology </em>, 73(2), 303-316. <a href="https://doi.org/10.1002/asi.24571" target="_blank" rel="noopener">https://doi.org/10.1002/asi.24571 </a></li>
  <li>Iliadis, A., &amp; Russo, F. (2016). Critical data studies: An introduction. <em>Big Data &amp; Society</em>, 3(2), 1-7. <a href="https://doi.org/10.1177/2053951716674238" target="_blank"
      rel="noopener">https://doi.org/10.1177/2053951716674238 </a></li>
  <li>Jaton, F. (2021). <em>The Constitution of Algorithms: Ground-Truthing, Programming, Formulating</em>. Cambridge, MA: MIT Press.</li>
  <li>Kitchin, R. (2021). <em>Data Lives</em>. Bristol, UK: Bristol University Press.</li>
  <li>Koopman, C. (2019). <em>How We Became Our Data: A Genealogy of the Informational Person. </em>Chicago, IL: University of Chicago Press.</li>
  <li>Thorp, J. (2021). <em>Living in Data: A Citizen's Guide to a Better Information Future</em>. New York, NY: MCD.</li>
</ul>
<h3 class="fs-5 mb-4"><strong>c. Methodologies for Reading Data</strong></h3>
<p class="fs-5 mb-4">This final subsection includes texts that deal more specifically with the different conceptualizations and methodologies through which datasets can be studied/read/analyzed.&nbsp;</p>
<ul>
  <li>boyd, d., &amp; Crawford, K. (2012). Critical Questions for Big Data: Provocations for a Cultural, Technological, and Scholarly Phenomenon. <em>Information, Communication &amp; Society, 15</em>(5), 662-679. <a
      href="https://doi.org/10.1080/1369118X.2012.678878" target="_blank" rel="noopener">https://doi.org/10.1080/1369118X.2012.678878</a></li>
  <li>Brock A. (2015). Deeper Data: A Response to boyd and Crawford. <em>Media, Culture &amp; Society, 37</em>(7):1084-1088. <a href="https://doi.org/10.1177/0163443715594105" target="_blank"
      rel="noopener">https://doi.org/10.1177/0163443715594105</a></li>
  <li>Driscoll, K., &amp; Walker, S. (2014). Working Within a Black Box: Transparency in the Collection and Production of Big Twitter Data. <em>International Journal of Communication, 8</em>, 1745&ndash;1764. <a
      href="https://ijoc.org/index.php/ijoc/article/view/2171/1159" target="_blank" rel="noopener">https://ijoc.org/index.php/ijoc/article/view/2171/1159</a></li>
  <li>Kitchin, R. (2014). <em>The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences</em>. London, UK: SAGE.</li>
  <li>Leonelli, S., &amp; Tempini, N. (Eds.). (2020). <em>Data Journeys in the Sciences</em>. Springer International Publishing.</li>
  <li>Malev&eacute;, N. (2020). On the Data Set&rsquo;s Ruins. <em>AI &amp; Society, 36</em>, 1117&ndash;1131. <a href="https://doi.org/10.1007/s00146-020-01093-w" target="_blank" rel="noopener">https://doi.org/10.1007/s00146-020-01093-w</a></li>
  <li>Metcalf, J., &amp; Crawford, K. (2016). Where Are Human Subjects in Big Data Research? The Emerging Ethics Divide. <em>Big Data &amp; Society, 3</em>(1), 1-14. <a href="https://doi.org/10.1177/2053951716650211" target="_blank"
      rel="noopener">https://doi.org/10.1177/2053951716650211</a></li>
  <li>Munk, A. K., Olesen, A. G., &amp; Jacomy, M. (2022). The Thick Machine: Anthropological AI Between Explanation and Explication. <em>Big Data &amp; Society, 9</em>(1), 1-14. <a href="https://doi.org/10.1177/20539517211069891" target="_blank"
      rel="noopener">https://doi.org/10.1177/20539517211069891</a></li>
  <li>Pasquale, F. (2021). Licensure as Data Governance. <em>Knight First Amendment Institute</em>. <a href="https://knightcolumbia.org/content/licensure-as-data-governance" target="_blank"
      rel="noopener">https://knightcolumbia.org/content/licensure-as-data-governance</a></li>
  <li>Poirier, L. (2021). Reading Datasets: Strategies for Interpreting the Politics of Data Signification. <em>Big Data &amp; Society, 8</em>(2), 1-19. <a href="https://doi.org/10.1177/20539517211029322" target="_blank"
      rel="noopener">https://doi.org/10.1177/20539517211029322</a></li>
  <li>Suchman, L., &amp; Trigg, R. H. (1993). Artificial Intelligence as Craftwork. In S. Chaiklin &amp; J. Lave (Eds.), <em>Understanding Practice</em> (pp. 144-178). New York, NY: Cambridge University Press.</li>
  <li>Zook, M., Barocas, S., boyd, d., Crawford, K., Keller, E., Gangadharan, S. P., Goodman, A., Hollander, R., Koenig, B. A., Metcalf, J., Narayanan, A., Nelson, A., &amp; Pasquale, F. (2017). Ten Simple Rules for Responsible Big Data Research.
    <em>PLOS Computational Biology, 13</em>(3), e1005399. <a href="https://doi.org/10.1371/journal.pcbi.1005399" target="_blank" rel="noopener">https://doi.org/10.1371/journal.pcbi.1005399</a></li>
</ul>


<h2 class="fs-5 mb-4" id="public_sources_of_datasets"><strong>3. PUBLIC SOURCES OF DATASETS&nbsp;</strong></h2>
<p class="fs-5 mb-4">While some datasets lie behind proprietary company walls, numerous datasets are available for public download. This section lists technical papers that accompany major public dataset releases, as well as popular repositories
  where disparate datasets are organized and made available to the broader public.</p>
<h3 class="fs-5 mb-4"><strong>a. Source Papers for Noteworthy Datasets&nbsp;</strong></h3>
<p class="fs-5 mb-4">New training datasets are typically accompanied by technical papers explaining the composition of the dataset and its potential applications. These papers often also include analyses of models using the new dataset and
  comparisons to similar existing datasets. There are infinitely more dataset source papers than can be included on this list; below is a sampling of the most highly cited and broadly influential releases.</p>
<ul>
  <li>Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., &amp; Schiele, B. (2016). The Cityscapes Dataset for Semantic Urban Scene Understanding. <em>Proceedings of the IEEE Conference on Computer Vision
      and Pattern Recognition</em>, 3213&ndash;3223. <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html" target="_blank"
      rel="noopener">https://openaccess.thecvf.com/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html</a></li>
  <li>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image Database.<em> 2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248&ndash;255. <a
      href="https://doi.org/10.1109/CVPR.2009.5206848" target="_blank" rel="noopener">https://doi.org/10.1109/CVPR.2009.5206848</a></li>
  <li>Geiger, A., Lenz, P., &amp; Urtasun, R. (2012). Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite. <em>2012 IEEE Conference on Computer Vision and Pattern Recognition</em>, 3354&ndash;3361. <a
      href="https://doi.org/10.1109/CVPR.2012.6248074" target="_blank" rel="noopener">https://doi.org/10.1109/CVPR.2012.6248074</a></li>
  <li>Huang, G. B., Mattar, M., Berg, T., &amp; Learned-Miller, E. (2008). Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. <em>Workshop on Faces in &ldquo;Real-Life&rdquo; Images: Detection,
      Alignment, and Recognition</em>. <a href="https://hal.inria.fr/inria-00321923" target="_blank" rel="noopener">https://hal.inria.fr/inria-00321923</a></li>
  <li>Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. <a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" target="_blank"
      rel="noopener">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a></li>
  <li>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-Based Learning Applied to Document Recognition. <em>Proceedings of the IEEE, 86</em>(11), 2278&ndash;2324. <a href="http://yann.lecun.com/exdb/publis/index.html#lecun-98"
      target="_blank" rel="noopener">http://yann.lecun.com/exdb/publis/index.html#lecun-98</a></li>
  <li>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll&aacute;r, P., &amp; Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. In D. Fleet, T. Pajdla, B. Schiele, &amp; T. Tuytelaars (Eds.), <em>Computer
      Vision &ndash; ECCV 2014</em> (pp. 740&ndash;755). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-10602-1_48" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-319-10602-1_48</a></li>
  <li>Marcus, M., Santorini, B., &amp; Marcinkiewicz, M. A. (1993). Building a Large Annotated Corpus of English: The Penn Treebank. <em>Technical Reports (CIS)</em>. <a href="https://repository.upenn.edu/cis_reports/237" target="_blank"
      rel="noopener">https://repository.upenn.edu/cis_reports/237</a></li>
  <li>Martin, D., Fowlkes, C., Tal, D., &amp; Malik, J. (2001). A Database of Human Segmented Natural Images and Its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics. <em>Proceedings Eighth IEEE International
      Conference on Computer Vision. ICCV 2001, 2</em>, 416&ndash;423. <a href="https://doi.org/10.1109/ICCV.2001.937655" target="_blank" rel="noopener">https://doi.org/10.1109/ICCV.2001.937655</a></li>
  <li>Miller, G. A. (1995). WordNet: A Lexical Database for English. <em>Communications of the ACM, 38</em>(11), 39&ndash;41. <a href="https://doi.org/10.1145/219717.219748" target="_blank" rel="noopener">https://doi.org/10.1145/219717.219748</a>
  </li>
  <li>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., &amp; Potts, C. (2013). Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. <em>Proceedings of the 2013 Conference on Empirical Methods in
      Natural Language Processing</em>, 1631&ndash;1642. <a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf" target="_blank" rel="noopener">https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf</a></li>
  <li>Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., &amp; Li, L.-J. (2016). YFCC100M: The New Data in Multimedia Research. <em>Communications of the ACM, 59</em>(2), 64&ndash;73. <a
      href="https://doi.org/10.1145/2812802" target="_blank" rel="noopener">https://doi.org/10.1145/2812802</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>b. Dataset Repositories</strong></h3>
<p class="fs-5 mb-4">These sites provide infrastructure for the organization, finding, and downloading of varying datasets.&nbsp;</p>
<ul>
  <li>Papers with Code: <a href="https://paperswithcode.com/datasets" target="_blank" rel="noopener">https://paperswithcode.com/datasets</a></li>
  <li>UCI Machine Learning Repository: <a href="https://archive.ics.uci.edu/ml/datasets.php" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/datasets.php</a></li>
  <li>Kaggle: <a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">https://www.kaggle.com/datasets</a></li>
  <li>Hugging Face: <a href="https://huggingface.co/datasets" target="_blank" rel="noopener">https://huggingface.co/datasets</a></li>
  <li>Google dataset search <a href="https://datasetsearch.research.google.com/" target="_blank" rel="noopener">https://datasetsearch.research.google.com/</a></li>
</ul>



<h2 class="fs-5 mb-4" id="studying_dataset_production"><strong>4. STUDYING DATASET PRODUCTION</strong></h2>
<p class="fs-5 mb-4">​​Training data requires significant human and computational effort to create. It is through this process of production that many of the effects of training data come to be shaped, from the processes of collection to labeling,
  deployment to deprecation. Texts in this section provide glimpses into the work behind datasets from varying angles, whether examining these production processes from a critical lens or describing the overall workflow of training data production
  from a technical standpoint.</p>
<h3 class="fs-5 mb-4"><strong>a. Sociotechnical / Critical Approaches to Labor of Training Data&nbsp;</strong></h3>
<p class="fs-5 mb-4">These texts draw on approaches and frameworks from science and technology studies, political economy, and labor studies to examine the production of training data from a critical lens, understanding how power relations are at
  work in this process.</p>
<ul>
  <li>Famularo, J., Hensellek, B., &amp; Walsh, P. (2021). Data Stewardship: A Letter to Computer Vision from Cultural Heritage Studies. <em>CVPR 2021</em>. <a
      href="https://www.academia.edu/49423941/Data_Stewardship_A_Letter_to_Computer_Vision_from_Cultural_Heritage_Studies?auto=citations&amp;from=cover_page" target="_blank"
      rel="noopener">https://www.academia.edu/49423941/Data_Stewardship_A_Letter_to_Computer_Vision_from_Cultural_Heritage_Studies?auto=citations&amp;from=cover_page</a></li>
  <li>Gray, M. L., &amp; Suri, S. (2019).<em>Ghost Work: How to Stop Silicon Valley from Building a New Global Underclass</em>, see &lsquo;Introduction: Ghosts in the Machine&rsquo; (pp. ix-xxxi) and &lsquo;1. Humans in the Loop&rsquo; (pp. 1-38).
    Houghton Mifflin Harcourt.</li>
  <li>Goetze, T. S., &amp; Abramson, D. (2021). Bigger Isn&rsquo;t Better: The Ethical and Scientific Vices of Extra-Large Datasets in Language Models. <em>WebSci</em>, pp. 69-75. <a href="https://doi.org/10.1145/3462741.3466809" target="_blank"
      rel="noopener">https://doi.org/10.1145/3462741.3466809&nbsp;</a></li>
  <li>Iliadis, A. (2019). The Tower of Babel problem: Making data make sense with Basic Formal Ontology. <em>Online Information Review</em>, 43(6), 1021–1045. <a href="https://doi.org/10.1108/OIR-07-2018-0210" target="_blank" rel="noopener">https://doi.org/10.1108/OIR-07-2018-0210 </a></li>
  <li>Jones, P. (2021, September 22). Refugees Help Power Machine Learning Advances at Microsoft, Facebook, and Amazon. <em>Rest of World</em>. <a href="https://restofworld.org/2021/refugees-machine-learning-big-tech/" target="_blank"
      rel="noopener">https://restofworld.org/2021/refugees-machine-learning-big-tech/</a></li>
  <li>Miceli, M., Schuessler, M., &amp; Yang, T. (2020). Between Subjectivity and Imposition: Power Dynamics in Data Annotation for Computer Vision. <em>Proceedings of the ACM on Human-Computer Interaction, 4(</em>CSCW2<em>)</em>, 1-25. <a
      href="https://doi.org/10.1145/3415186" target="_blank" rel="noopener">https://doi.org/10.1145/3415186</a></li>
  <li>Newlands, G. (2021). Lifting the Curtain: Strategic Visibility of Human Labour in AI-as-a-Service. <em>Big Data &amp; Society</em>, <em>8</em>(1), 1-14. <a
      href="https://urldefense.com/v3/__https://doi.org/10.1177/20539517211016026__;!!LIr3w8kk_Xxm!8b-dYz-Rsa7iMiBfoUZfFcOUl8g_my0VNHnOmE-6dmTJHkV49S2av1if6vAHzT2B$" target="_blank" rel="noopener">https://doi.org/10.1177/20539517211016026</a></li>
  <li>Sachs, S. E. (2020). The Algorithm At Work? Explanation and Repair in the Enactment of Similarity in Art Data.<em> Information, Communication &amp; Society, 23</em>(11), 1689&ndash;1705. <a href="https://doi.org/10.1080/1369118X.2019.1612933"
      target="_blank" rel="noopener">https://doi.org/10.1080/1369118X.2019.1612933</a></li>
  <li>Sambasivan, N. (2021). Seeing Like a Dataset from the Global South. <em>Interactions, 28</em>(4), 76&ndash;78. <a href="https://doi.org/10.1145/3466160" target="_blank" rel="noopener">https://doi.org/10.1145/3466160</a></li>
  <li>Sap, M., Card, D., Gabriel, S., Choi, Y., &amp; Smith, N. A. (2019). The Risk of Racial Bias in Hate Speech Detection. <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 1668&ndash;1678. <a
      href="https://doi.org/10.18653/v1/P19-1163" target="_blank" rel="noopener">https://doi.org/10.18653/v1/P19-1163</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>b. Organizational Workflows in Dataset Production&nbsp;</strong></h3>
<p class="fs-5 mb-4">Texts included here look to training data production from a practitioner-oriented lens. They survey either the entire workflow of training data production or specific stages within this process to identify challenges and suggest
  best practices.</p>
<ul>
  <li>Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan, N., Nushi, B., &amp; Zimmermann, T. (2019). Software Engineering for Machine Learning: A Case Study. <em>2019 IEEE/ACM 41st International Conference on Software
      Engineering: Software Engineering in Practice (ICSE-SEIP)</em>, 291&ndash;300. <a href="https://doi.org/10.1109/ICSE-SEIP.2019.00042" target="_blank" rel="noopener">https://doi.org/10.1109/ICSE-SEIP.2019.00042</a></li>
  <li>Ashmore, R., Calinescu, R., &amp; Paterson, C. (2019). Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1905.04223" target="_blank"
      rel="noopener">http://arxiv.org/abs/1905.04223</a></li>
  <li>Barclay, I., Taylor, H., Preece, A., Taylor, I., Verma, D., &amp; de Mel, G. (2020). A Framework for Fostering Transparency in Shared Artificial Intelligence Models by Increasing Visibility of Contributions. <em>Concurrency and Computation:
      Practice and Experience, 33</em>(19), e6129. <a href="https://doi.org/10.1002/cpe.6129" target="_blank" rel="noopener">https://doi.org/10.1002/cpe.6129</a></li>
  <li>Bhardwaj, A., Bhattacherjee, S., Chavan, A., Deshpande, A., Elmore, A. J., Madden, S., &amp; Parameswaran, A. G. (2014). DataHub: Collaborative Data Science &amp; Dataset Version Management at Scale. <em>ArXiv</em>. <a
      href="http://arxiv.org/abs/1409.0798" target="_blank" rel="noopener">http://arxiv.org/abs/1409.0798</a></li>
  <li>Chandrabose, A., &amp; Chakravarthi, B. R. (2021). An Overview of Fairness in Data &ndash; Illuminating the Bias in Data Pipeline. <em>LTEDI</em>. <a href="https://aclanthology.org/2021.ltedi-1.5" target="_blank"
      rel="noopener">https://aclanthology.org/2021.ltedi-1.5</a></li>
  <li>Dong, W., &amp; Fu, W.-T. (2010). Cultural Difference in Image Tagging. <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>, 981&ndash;984. <a href="https://doi.org/10.1145/1753326.1753472" target="_blank"
      rel="noopener">https://doi.org/10.1145/1753326.1753472</a></li>
  <li>Hanley, M., Khandelwal, A., Averbuch-Elor, H., Snavely, N., &amp; Nissenbaum, H. (2020). An Ethical Highlighter for People-Centric Dataset Creation. <em>ArXiv</em>. <a href="http://arxiv.org/abs/2011.13583" target="_blank"
      rel="noopener">http://arxiv.org/abs/2011.13583</a></li>
  <li>Hutchinson, B., Smart, A., Hanna, A., Denton, E., Greer, C., Kjartansson, O., Barnes, P., &amp; Mitchell, M. (2021). Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure. <em>ArXiv</em>. <a
      href="http://arxiv.org/abs/2010.13561" target="_blank" rel="noopener">http://arxiv.org/abs/2010.13561</a></li>
  <li>Geiger, R., Cope, D., Ip, J., Lotosh, M., Shah, A., Weng, J., &amp; Tang, R. (2021). &ldquo;Garbage In, Garbage Out&rdquo; Revisited: What Do Machine Learning Application Papers Report About Human-Labeled Training Data? <em>ArXiv</em>. <a
      href="https://doi.org/10.1162/qss_a_00144" target="_blank" rel="noopener">https://doi.org/10.1162/qss_a_00144</a></li>
  <li>Holstein, K., Vaughan, J. W., Daum&eacute; III, H., Dud&iacute;k, M., &amp; Wallach, H. (2019). Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need? <em>Proceedings of the 2019 CHI Conference on Human Factors in
      Computing Systems</em>, 1&ndash;16. <a href="https://doi.org/10.1145/3290605.3300830" target="_blank" rel="noopener">https://doi.org/10.1145/3290605.3300830</a></li>
  <li>Muller, M. J., Wolf, C. T., Andres, J., Desmond, M., Joshi, N. N., Ashktorab, Z., Sharma, A., Brimijoin, K., Pan, Q., Duesterwald, E., &amp; Dugan, C. (2021). Designing Ground Truth and the Social Life of Labels. <em>Proceedings of the 2021 CHI
      Conference on Human Factors in Computing Systems</em>, 1-16. <a href="https://doi.org/10.1145/3411764.3445402" target="_blank" rel="noopener">https://doi.org/10.1145/3411764.3445402</a></li>
  <li>Polyzotis, N., Roy, S., Whang, S. E., &amp; Zinkevich, M. (2018). Data Lifecycle Challenges in Production Machine Learning: A Survey. <em>ACM SIGMOD Record</em>, <em>47</em>(2), 17&ndash;28. <a href="https://doi.org/10.1145/3299887.3299891"
      target="_blank" rel="noopener">https://doi.org/10.1145/3299887.3299891</a></li>
  <li>Roh, Y., Heo, G., &amp; Whang, S. E. (2021). A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective. <em>IEEE Transactions on Knowledge and Data Engineering, 33</em>(4), 1328&ndash;1347. <a
      href="https://doi.org/10.1109/TKDE.2019.2946162" target="_blank" rel="noopener">https://doi.org/10.1109/TKDE.2019.2946162</a></li>
  <li>Tatman, R. (2018). Setting Up Your Public Data for Success. <em>2018 IEEE International Conference on Big Data (Big Data)</em>, 3261&ndash;3262. <a href="https://doi.org/10.1109/BigData.2018.8622190" target="_blank"
      rel="noopener">https://doi.org/10.1109/BigData.2018.8622190</a></li>
  <li>Sachdeva, P. S., Barreto, R., von Vacano, C., & Kennedy, C. J. (2022). Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1585–1603. <a href="https://doi.org/10.1145/3531146.3533216" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533216</a> </li>
  <li>Sambasivan, N., &amp; Veeraraghavan, R. (2022). The Deskilling of Domain Expertise in AI Development. <em>Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, 1-14. <a href="https://doi.org/10.1145/3491102.3517578" target="_blank" rel="noopener">https://doi.org/10.1145/3491102.3517578 </a></li>
  <li>Shanmugam, D., Diaz, F., Shabanian, S., Funck, M., & Biega, A. (2022). Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 839-849. <a href="https://doi.org/10.1145/3531146.3533148" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533148</a> </li>
  <li>Vaughan, J. W. (2018). Making Better Use of the Crowd: How Crowdsourcing Can Advance Machine Learning Research. <em>Journal of Machine Learning Research, 18</em>(193), 1&ndash;46. <a href="https://dl.acm.org/doi/10.5555/3122009.3242050"
      target="_blank" rel="noopener">https://dl.acm.org/doi/10.5555/3122009.3242050</a></li>
  <li>Wang, D., Prabhat, S., &amp; Sambasivan, N. (2022). Whose AI Dream? In search of the aspiration in data annotation. <em>Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</em>, 1-16. <a href="https://doi.org/10.1145/3491102.3502121" target="_blank"
      rel="noopener">https://doi.org/10.1145/3491102.3502121 </a></li>
</ul>



<h2 class="fs-5 mb-4" id="analyses_of_training_datasets"><strong>5. ANALYSES OF TRAINING DATASETS</strong></h2>
<p class="fs-5 mb-4">This section highlights works that analyze training datasets from a variety of methodological and theoretical perspectives. While we understand that many of the titles that span across the major headings in this reading list
  involve some form of &ldquo;dataset analysis,&rdquo; we highlight in this particular section studies in which the analysis itself comprises the thrust of the article/chapter/work. The works in this section focus primarily on the <em>details of the
    analysis</em> as opposed to conducting an analysis as a preliminary step to introduce a more central argument or intervention.&nbsp;</p>
<h3 class="fs-5 mb-4"><strong>a. Sociotechnical &amp; Critical Studies</strong></h3>
<p class="fs-5 mb-4">This subsection focuses on articles and chapters that approach their analyses of training datasets grounded in frameworks primarily taken from critical studies or science and technology studies.&nbsp;</p>
<ul>
  <li>Bao, M., Zhou, A., Zottola, S. A., Brubach, B., Desmarais, S., Horowitz, A., Lum, K., &amp; Venkatasubramanian, S. (2021). It&rsquo;s COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks.
    <em>ArXiv</em>. <a href="https://arxiv.org/abs/2106.05498" target="_blank" rel="noopener">https://arxiv.org/abs/2106.05498</a></li>
  <li>Busch, L. (2014). A Dozen Ways to Get Lost in Translation: Inherent Challenges in Large Scale Data Sets. <em>International Journal of Communication</em>, <em>8</em>, 1727-1744. <a href="https://ijoc.org/index.php/ijoc/article/view/2160"
      target="_blank" rel="noopener">https://ijoc.org/index.php/ijoc/article/view/2160</a></li>
  <li>Coleman, C. N. (2020). Managing Bias When Library Collections Become Data. <em>International Journal of Librarianship, 5</em>(1), 8&ndash;19. <a href="https://doi.org/10.23974/ijol.2020.vol5.1.162" target="_blank" rel="noopener">
      https://doi.org/10.23974/ijol.2020.vol5.1.162</a></li>
  <li>Coveney, P. V., Dougherty, E. R., &amp; Highfield, R. R. (2016). Big Data Need Big Theory Too. <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374</em>, 1-11. <a
      href="https://doi.org/10.1098/rsta.2016.0153" target="_blank" rel="noopener">https://doi.org/10.1098/rsta.2016.0153</a></li>
  <li>Feinberg, M. (2017). A Design Perspective on Data. <em>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em>, 2952&ndash;2963. <a href="https://doi.org/10.1145/3025453.3025837" target="_blank"
      rel="noopener">https://doi.org/10.1145/3025453.3025837</a></li>
  <li>Jo, E. S., &amp; Gebru, T. (2020). Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning. <em>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</em>, 306&ndash;316. <a
      href="https://doi.org/10.1145/3351095.3372829" target="_blank" rel="noopener">https://doi.org/10.1145/3351095.3372829</a></li>
  <li>Prabhu, V. U., &amp; Birhane, A. (2020). Large Image Datasets: A Pyrrhic Win for Computer Vision? <em>ArXiv</em>. <a href="http://arxiv.org/abs/2006.16923" target="_blank" rel="noopener">http://arxiv.org/abs/2006.16923</a></li>
  <li>Richardson, R., Schultz, J. M., &amp; Crawford, K. (2019). Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice. <em>NYU Law Review</em>, <em>94</em>(15), 15&ndash;55. <a
      href="https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights-violations-impact-police-data-predictive-policing-systems-and-justice/" target="_blank"
      rel="noopener">https://www.nyulawreview.org/online-features/dirty-data-bad-predictions-how-civil-rights-violations-impact-police-data-predictive-policing-systems-and-justice/</a></li>
  <li>Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P. K., &amp; Aroyo, L. (2021). &ldquo;Everyone Wants to Do the Model Work, Not the Data Work&rdquo;: Data Cascades in High-Stakes AI. <em>Proceedings of the 2021 CHI Conference on
      Human Factors in Computing Systems</em>, 1-15. <a href="https://doi.org/10.1145/3411764.3445518" target="_blank" rel="noopener">https://doi.org/10.1145/3411764.3445518</a></li>
  <li>Scheuerman, M. K., Denton, E., &amp; Hanna, A. (2021). Do Datasets Have Politics? Disciplinary Values in Computer Vision Dataset Development. <em>ArXiv</em>. <a href="https://doi.org/10.1145/3476058" target="_blank"
      rel="noopener">https://doi.org/10.1145/3476058</a></li>
  <li>Scheuerman, M. K., Paul, J. M., &amp; Brubaker, J. R. (2019). How Computers See Gender: An Evaluation of Gender Classification in Commercial Facial Analysis Services.<em> Proceedings of the ACM on Human-Computer Interaction, 3(CSCW)</em>, 1-33.
    <a href="https://doi.org/10.1145/3359246" target="_blank" rel="noopener">https://doi.org/10.1145/3359246</a></li>
  <li>Scheuerman, M. K., Wade, K., Lustig, C., &amp; Brubaker, J. R. (2020). How We&rsquo;ve Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis. <em>Proceedings of the ACM on Human-Computer
      Interaction, 4(CSCW1)</em>, 1-35. <a href="https://doi.org/10.1145/3392866" target="_blank" rel="noopener">https://doi.org/10.1145/3392866</a></li>
  <li>Smits, T., &amp; Wevers, M. (2021). The Agency of Computer Vision Models as Optical Instruments. <em>Visual Communication</em>, 1-21. <a href="https://doi.org/10.1177/1470357221992097" target="_blank"
      rel="noopener">https://doi.org/10.1177/1470357221992097</a></li>
  <li>Stevens, N., &amp; Keyes, O. (2021). Seeing infrastructure: Race, Facial Recognition and the Politics of Data. <em>Cultural Studies, 35</em>(4-5), 833-853. <a href="https://doi.org/10.1080/09502386.2021.1895252" target="_blank"
      rel="noopener">https://doi.org/10.1080/09502386.2021.1895252</a></li>
  <li>Trewin, S. (2018). AI Fairness for People with Disabilities: Point of View. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1811.10670" target="_blank" rel="noopener">http://arxiv.org/abs/1811.10670</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>b. Technical Approaches to Studying Datasets</strong></h3>
<p class="fs-5 mb-4">Here, we introduce works that detail &ldquo;technical&rdquo; methods for the study of datasets. While the titles housed under the following subsection 5c, &ldquo;Technical Audits,&rdquo; deal with the <em>investigative</em>
  technical analysis of <em>particular</em> datasets, the works in this subsection are more concerned with introducing technical methods to approach the study of datasets and their particular components. Many of these studies do contain audit-style
  analyses, but we differentiate them from subsection 5c because their focus is on introducing or using technical methods for dataset analysis in general, as opposed to dissecting various components of particular datasets.&nbsp;</p>
<ul>
  <li>Balayn, A., Kulynych, B., &amp; Guerses, S. (2021). Exploring Data Pipelines through the Process Lens: A Reference Model for Computer Vision. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2107.01824" target="_blank"
      rel="noopener">https://arxiv.org/abs/2107.01824</a></li>
  <li>Bender, E. M., Gebru, T., McMillan-Major, A., &amp; Mitchell, M. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. <em>FAccT</em>. <a href="https://doi.org/10.1145/3442188.3445922" target="_blank"
      rel="noopener">https://doi.org/10.1145/3442188.3445922</a></li>
  <li>Blodgett, S. L., Lopez, G., Olteanu, A., Sim, R., &amp; Wallach, H. (202). Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets. <em>Proceedings of the 59th Annual Meeting of the Association for Computational
      Linguistics</em>, <em>1</em>, 1004-1015.&nbsp;</li>
  <li>Cheng, V., Suriyakumar, V., Dullerud, N., Joshi, S., &amp; Ghassemi, M. (2021). Can You Fake It Until You Make It?: Impacts of Differentially Private Synthetic Data on Downstream Classification Fairness. <em>Proceedings of the 2021 ACM
      Conference on Fairness, Accountability, and Transparency</em>, 149-160. <a href="https://doi.org/10.1145/3442188.3445879" target="_blank" rel="noopener"> https://doi.org/10.1145/3442188.3445879</a></li>
  <li>Gardner, M., Merrill, W., Dodge, J., Peters, M. E., Ross, A., Singh, S., &amp; Smith, N. A. (2021). Competency Problems: On Finding and Removing Artifacts in Language Data. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2104.08646"
      target="_blank" rel="noopener">https://arxiv.org/abs/2104.08646</a></li>
  <li>Fabbrizzi, S., Papadopoulos, S., Ntoutsi, E., &amp; Kompatsiaris, Y. (2021). A Survey on Bias in Visual Datasets. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2107.07919" target="_blank" rel="noopener">https://arxiv.org/abs/2107.07919</a>
  </li>
  <li>Hirota, Y., Nakashima, Y., & Garcia, N. (2022). Gender and Racial Bias in Visual Question Answering Datasets. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1280–1292. <a href="https://doi.org/10.1145/3531146.3533184" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533184</a> </li>
  <li>Hutchinson, B., Rostamzadeh, N., Greer, C., Heller, K., & Prabhakaran, V. (2022). Evaluation Gaps in Machine Learning Practice. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1859–1876. <a href="https://doi.org/10.1145/3531146.3533233" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533233</a> </li>
  <li>Jung, T., Kang, D., Mentch, L., &amp; Hovy, E. (2019). Earlier Isn&rsquo;t Always Better: Sub-aspect Analysis on Corpus and System Biases in Summarization. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1908.11723" target="_blank"
      rel="noopener">http://arxiv.org/abs/1908.11723</a></li>
  <li>Kilgarriff, A., &amp; Grefenstette, G. (2003). Introduction to the Special Issue on the Web as Corpus. <em>Computational Linguistics</em>, <em>29</em>(3), 333&ndash;348. <a href="https://doi.org/10.1162/089120103322711569" target="_blank"
      rel="noopener">https://doi.org/10.1162/089120103322711569</a></li>
  <li>Koesten, L., Vougiouklis, P., Simperl, E., &amp; Groth, P. (2020). Dataset Reuse: Toward Translating Principles to Practice. <em>Patterns, 1</em>(8), 100136. <a href="https://doi.org/10.1016/j.patter.2020.100136" target="_blank"
      rel="noopener">https://doi.org/10.1016/j.patter.2020.100136</a></li>
  <li>Laranjeira da Silva, C., Macedo, J., Avila, S., & dos Santos, J. (2022). Seeing without Looking: Analysis Pipeline for Child Sexual Abuse Datasets. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2189–2205. <a href="https://doi.org/10.1145/3531146.3534636" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3534636</a></li>
  <li>Madras, D., Creager, E., Pitassi, T., &amp; Zemel, R. (2019). Fairness through Causal Awareness: Learning Causal Latent-Variable Models for Biased Data. <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>,
    349&ndash;358. <a href="https://doi.org/10.1145/3287560.3287564" target="_blank" rel="noopener">https://doi.org/10.1145/3287560.3287564</a></li>
  <li>Moreno-Torres, J. G., Raeder, T., Alaiz-Rodr&iacute;guez, R., Chawla, N. V., &amp; Herrera, F. (2012). A Unifying View on Dataset Shift in Classification. <em>Pattern Recognition</em>, <em>45</em>(1), 521&ndash;530. <a
      href="https://doi.org/10.1016/j.patcog.2011.06.019" target="_blank" rel="noopener">https://doi.org/10.1016/j.patcog.2011.06.019</a></li>
  <li>Olson, R. S., La Cava, W., Orzechowski, P., Urbanowicz, R. J., &amp; Moore, J. H. (2017). PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison. <em>BioData Mining</em>, <em>10</em>(36). <a
      href="https://urldefense.com/v3/__https://doi.org/10.1186/s13040-017-0154-4__;!!LIr3w8kk_Xxm!8b-dYz-Rsa7iMiBfoUZfFcOUl8g_my0VNHnOmE-6dmTJHkV49S2av1if6lTq4Vgs$" target="_blank" rel="noopener">https://doi.org/10.1186/s13040-017-0154-4</a></li>
  <li>Rabanser, S., G&uuml;nnemann, S., &amp; Lipton, Z. C. (2019). Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1810.11953" target="_blank"
      rel="noopener">http://arxiv.org/abs/1810.11953</a></li>
  <li>Rieke, A., Sutherland, V., Svirsky, D., & Hsu, M. (2022). Imperfect Inferences: A Practical Assessment. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 767-777. <a href="https://doi.org/10.1145/3531146.3533140" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533140</a> </li>
  <li>Straw, I., &amp; Callison-Burch, C. (2020). Artificial Intelligence in Mental Health and the Biases of Language Based Models. <em>PLOS ONE, 15</em>(12), e0240376. <a href="https://doi.org/10.1371/journal.pone.0240376" target="_blank"
      rel="noopener">https://doi.org/10.1371/journal.pone.0240376</a></li>
  <li>Welty, C., Paritosh, P., &amp; Aroyo, L. (2019). Metrology for AI: From Benchmarks to Instruments. <em>ArXiv</em>. <a href="https://arxiv.org/abs/1911.01875v1" target="_blank" rel="noopener">https://arxiv.org/abs/1911.01875v1</a></li>
  <li>Wesley, A. M., &amp; Matisziw, T. C. (2021). Methods for Measuring Geodiversity in Large Overhead Imagery Datasets.<em> IEEE Access, 9</em>, 100279&ndash;100293. <a href="https://doi.org/10.1109/ACCESS.2021.3096034" target="_blank"
      rel="noopener">https://doi.org/10.1109/ACCESS.2021.3096034</a></li>
  <li>Zanella-B&eacute;guelin, S., Wutschitz, L., Tople, S., R&uuml;hle, V., Paverd, A., Ohrimenko, O., K&ouml;pf, B., &amp; Brockschmidt, M. (2020). Analyzing Information Leakage of Updates to Natural Language Models. <em>Proceedings of the 2020 ACM
      SIGSAC Conference on Computer and Communications Security</em>, 363&ndash;375. <a href="https://doi.org/10.1145/3372297.3417880" target="_blank" rel="noopener">https://doi.org/10.1145/3372297.3417880</a></li>
  <li>Zhong, R., Chen, Y., Patton, D., Selous, C., &amp; McKeown, K. (2019). Detecting and Reducing Bias in a High Stakes Domain. <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
      Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, 4765&ndash;4775. <a href="https://doi.org/10.18653/v1/D19-1483" target="_blank" rel="noopener">https://doi.org/10.18653/v1/D19-1483</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>c. Technical Audits</strong></h3>
<p class="fs-5 mb-4">This subsection includes works that employ technical audit-style investigations (e.g., Buolamwini &amp; Gebru, 2018; Raji et al, 2020) of particular datasets.&nbsp;</p>
<ul>
  <li>Babaeianjelodar, M., Lorenz, S., Gordon, J., Matthews, J., &amp; Freitag, E. (2020). Quantifying Gender Bias in Different Corpora.<em> Companion Proceedings of the Web Conference 2020</em>, 752&ndash;759. <a
      href="https://doi.org/10.1145/3366424.3383559" target="_blank" rel="noopener">https://doi.org/10.1145/3366424.3383559</a></li>
  <li>Bountouridis, D., Makhortykh, M., Sullivan, E., Harambam, J., Tintarev, N., &amp; Hauff, C. (2019). Annotating Credibility: Identifying and Mitigating Bias in Credibility Datasets. <em>ROME 2019 - Workshop on Reducing Online Misinformation
      Exposure</em>. <a href="https://rome2019.github.io/papers/Bountouridis_etal_ROME2019.pdf" target="_blank" rel="noopener">https://rome2019.github.io/papers/Bountouridis_etal_ROME2019.pdf</a></li>
  <li>Buolamwini, J., &amp; Gebru, T. (2018, January). Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. <em>Conference on fairness, accountability and transparency</em>, 77-91. <a
      href="https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/" target="_blank"
      rel="noopener">https://www.media.mit.edu/publications/gender-shades-intersectional-accuracy-disparities-in-commercial-gender-classification/</a></li>
  <li>Costanza-Chock, S., Raji, I. D., & Buolamwini, J. (2022). Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1571–1583. <a href="https://doi.org/10.1145/3531146.3533213" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533213</a> </li>
  <li>Davidson, T., Bhattacharya, D., &amp; Weber, I. (2019). Racial Bias in Hate Speech and Abusive Language Detection Datasets. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1905.12516" target="_blank"
      rel="noopener">http://arxiv.org/abs/1905.12516</a></li>
  <li>​​Dulhanty, C., &amp; Wong, A. (2019). Auditing ImageNet: Towards a Model-driven Framework for Annotating Demographic Attributes of Large-Scale Image Datasets. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1905.01347" target="_blank"
      rel="noopener">http://arxiv.org/abs/1905.01347</a></li>
  <li>Dulhanty, C., &amp; Wong, A. (2020). Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification. <em>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</em>, 244&ndash;250. <a
      href="https://doi.org/10.1145/3375627.3375875" target="_blank" rel="noopener"> https://doi.org/10.1145/3375627.3375875</a></li>
  <li>Dulhanty, C. (2020). Issues in Computer Vision Data Collection: Bias, Consent, and Label Taxonomy [University of Waterloo]. <a href="https://uwspace.uwaterloo.ca/handle/10012/16414" target="_blank"
      rel="noopener">https://uwspace.uwaterloo.ca/handle/10012/16414</a></li>
  <li>Heinzerling, B. (2019, July 21). NLP&rsquo;s Clever Hans Moment has Arrived. <em>Benjamin Heinzerling</em>. <a href="https://bheinzerling.github.io/post/clever-hans/" target="_blank"
      rel="noopener">https://bheinzerling.github.io/post/clever-hans/</a></li>
  <li>Hutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y., &amp; Denuyl, S. (2020). Social Biases in NLP Models as Barriers for Persons with Disabilities. <em>ArXiv</em>. <a href="http://arxiv.org/abs/2005.00813" target="_blank"
      rel="noopener">http://arxiv.org/abs/2005.00813</a></li>
  <li>Klockmann, V., von Schenk, A., &amp; Villeval, M. C. (2021). Artificial Intelligence, Ethics, and Diffused Pivotality. <em>Working Paper Series, GATE</em>. <a href="https://ssrn.com/abstract=3853829" target="_blank"
      rel="noopener">https://ssrn.com/abstract=3853829</a></li>
  <li>Luccioni, A., &amp; Viviano, J. (2021). What&rsquo;s in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Corpus. <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
      International Joint Conference on Natural Language Processing</em>, 182-189. <a href="https://aclanthology.org/2021.acl-short.24.pdf" target="_blank" rel="noopener">https://aclanthology.org/2021.acl-short.24.pdf</a></li>
  <li>Mecati, M., Cannav&ograve;, F. E., Vetr&ograve;, A., &amp; Torchiano, M. (2020). Identifying Risks in Datasets for Automated Decision&ndash;Making. In G. Viale Pereira, M. Janssen, H. Lee, I. Lindgren, M. P. Rodr&iacute;guez Bol&iacute;var, H.
    J. Scholl, &amp; A. Zuiderwijk (Eds.), <em>Electronic Government</em> (pp. 332&ndash;344). Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-57599-1_25" target="_blank"
      rel="noopener">https://doi.org/10.1007/978-3-030-57599-1_25</a></li>
  <li>Raji, I. D., &amp; Fried, G. (2021). About Face: A Survey of Facial Recognition Evaluation. <em>ArXiv</em>. <a href="http://arxiv.org/abs/2102.00813" target="_blank" rel="noopener">http://arxiv.org/abs/2102.00813</a></li>
  <li>Raji, I. D., Gebru, T., Mitchell, M., Buolamwini, J., Lee, J., &amp; Denton, E. (2020). Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing. <em>ArXiv</em>. <a href="http://arxiv.org/abs/2001.00964" target="_blank"
      rel="noopener">http://arxiv.org/abs/2001.00964</a></li>
  <li>Rambachan, A., &amp; Roth, J. (2020). Bias In, Bias Out? Evaluating the Folk Wisdom. <em>ArXiv</em>. <a href="https://doi.org/10.4230/LIPIcs.FORC.2020.6" target="_blank" rel="noopener">https://doi.org/10.4230/LIPIcs.FORC.2020.6</a></li>
  <li>Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson, J., &amp; Sculley, D. (2017). No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World. <em>ArXiv</em>. <a
      href="https://arxiv.org/abs/1711.08536" target="_blank" rel="noopener">https://arxiv.org/abs/1711.08536</a></li>
  <li>Vidgen, B., &amp; Derczynski, L. (2020). Directions in Abusive Language Training Data: Garbage In, Garbage Out. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2004.01670" target="_blank" rel="noopener">https://arxiv.org/abs/2004.01670</a></li>
  <li>Wang, T., Zhao, J., Yatskar, M., Chang, K.-W., &amp; Ordonez, V. (2019). Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image Representations. <em>2019 IEEE/CVF International Conference on Computer Vision
      (ICCV)</em>. <a href="https://doi.org/10.1109/ICCV.2019.00541" target="_blank" rel="noopener">https://doi.org/10.1109/ICCV.2019.00541</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>d. Visual &amp; Artistic Approaches to Datasets</strong></h3>
<p class="fs-5 mb-4">​​This final subsection assembles artistic and visual approaches/formats for the analysis of datasets.</p>
<ul>
  <li>Baker, D. (2022). <em>Datasets Have Worldviews</em> [Website]. PAIR Explorables. <a href="https://pair.withgoogle.com/explorables/dataset-worldviews/" target="_blank"
      rel="noopener">https://pair.withgoogle.com/explorables/dataset-worldviews/</a></li>
  <li>Crawford, K. &amp; Paglen, T. (2019). <em>Training Humans</em> [Large-scale exhibition]. Fondazione Prada, Milan, 2019-2020. <a href="https://www.fondazioneprada.org/project/training-humans/?lang=en" target="_blank"
      rel="noopener">https://www.fondazioneprada.org/project/training-humans/?lang=en</a>Publication: <a href="https://bookshop.fondazioneprada.org/shop/UIShop/Shop_DettaglioArticolo.aspx?idshoparticolo=117" target="_blank" rel="noopener">Training
      Humans Book</a></li>
  <li>Dewey-Hagbord, H. (2019). <em>​​How Do You See Me? </em>[<span style="color: #222222;">Adversarial processes]. The Photographer&rsquo;s Gallery, London, UK. </span><a
      href="https://thephotographersgallery.org.uk/whats-on/heather-dewey-hagborg-how-do-you-see-me" target="_blank" rel="noopener">https://thephotographersgallery.org.uk/whats-on/heather-dewey-hagborg-how-do-you-see-me</a></li>
  <li>Malev&eacute;, N. (2019).<em>12 hours of ImageNet</em> [Computer script]. <span style="color: #222222;">The Photographer&rsquo;s Gallery, London, UK. </span><a href="https://thephotographersgallery.org.uk/whats-on/exhibiting-imagenet"
      target="_blank" rel="noopener">https://thephotographersgallery.org.uk/whats-on/exhibiting-imagenet</a></li>
  <li>Paglen, T. and Crawford, K. (2019). <em>Imagenet Roulette</em> [Software program]. Launched at SXSW. <a href="https://www.youtube.com/watch?v=S0yEPZJnvgs" target="_blank" rel="noopener">https://www.youtube.com/watch?v=S0yEPZJnvgs</a></li>
  <li>Pipkin, E. (2020). <em>On Lacework: Watching an Entire Machine-Learning Dataset. Unthinking Photography</em>. <a href="https://unthinking.photography/articles/on-lacework" target="_blank"
      rel="noopener">https://unthinking.photography/articles/on-lacework</a></li>
  <li>Ridler, A. (2018). <em>Myriad (Tulips)</em> [C-type digital prints with handwritten annotations, magnetic paint, magnets]. Barbican Centre, London, UK. <a href="http://annaridler.com/myriad-tulips" target="_blank"
      rel="noopener">http://annaridler.com/myriad-tulips</a></li>
</ul>



<h2 class="fs-5 mb-4" id="responses_to_dataset_problems"><strong>6. RESPONSES TO DATASET PROBLEMS</strong></h2>
<p class="fs-5 mb-4">Here we assemble literature that proposes responses to commonly identified sociotechnical problems with ML datasets. Most of the articles in this vein focused on technical responses to addressing bias (writ broadly), while a few
  address other concerns such as privacy and security. We do not necessarily endorse these approaches; rather, this is a loose mapping of emerging areas of focus in response to problems. Note that there is some overlap with the readings suggested in
  Section 5, as many of these papers investigate particular datasets; however, the papers listed here emphasize approaches to addressing specific problems.&nbsp;</p>
<h3 class="fs-5 mb-4"><strong>a. General Recommendations for Dataset Design</strong></h3>
<p class="fs-5 mb-4">This subsection covers miscellaneous broad recommendations for the creation of fairer and more accountable datasets.&nbsp;</p>
<ul>
  <li>Andrus, M., & Villeneuve, S. (2022). Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1709–1721. <a href="https://doi.org/10.1145/3531146.3533226" target="_blank" rel="noopener">
      https://doi.org/10.1145/3531146.3533226</a> </li>
  <li>Bilstrup, K.-E. K., Kaspersen, M. H., Assent, I., Enni, S., & Petersen, M. G. (2022). From Demo to Design in Teaching Machine Learning. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2168–2178. <a href="https://doi.org/10.1145/3531146.3534634" target="_blank" rel="noopener">
      https://doi.org/10.1145/3531146.3534634</a> </li>
  <li>Bowman, S. R., &amp; Dahl, G. E. (2021). What Will it Take to Fix Benchmarking in Natural Language Understanding?<em> NAACL</em>. <a href="https://doi.org/10.18653/V1/2021.NAACL-MAIN.385" target="_blank" rel="noopener">
      https://doi.org/10.18653/V1/2021.NAACL-MAIN.385</a></li>
  <li>Boyd, K. (2022). Designing Up with Value-Sensitive Design: Building a Field Guide for Ethical ML Development. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2069–2082. <a href="https://doi.org/10.1145/3531146.3534626" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3534626</a> </li>
  <li>Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G., Singh, A., Ringshia, P., Ma, Z., Thrush, T., Riedel, S., Waseem, Z., Stenetorp, P., Jia, R., Bansal, M., Potts, C., &amp; Williams, A. (2021). Dynabench:
    Rethinking Benchmarking in NLP. <em>NAACL</em>. <a href="https://doi.org/10.18653/V1/2021.NAACL-MAIN.324" target="_blank" rel="noopener">https://doi.org/10.18653/V1/2021.NAACL-MAIN.324</a></li>
  <li>Panch, T., Pollard, T. J., Mattie, H., Lindemer, E., Keane, P. A., &amp; Celi, L. A. (2020). &ldquo;Yes, But Will It Work for My Patients?&rdquo; Driving Clinically Relevant Research with Benchmark Datasets. <em>Npj Digital Medicine</em>,
    <em>3</em>(1), 1&ndash;4. <a href="https://doi.org/10.1038/s41746-020-0295-6" target="_blank" rel="noopener">https://doi.org/10.1038/s41746-020-0295-6</a></li>
  <li>Peng, K., Mathur, A., &amp; Narayanan, A. (2021). Mitigating Dataset Harms Requires Stewardship: Lessons from 1000 Papers. <em>ArXiv</em>. <a href="http://arxiv.org/abs/2108.02922" target="_blank"
      rel="noopener">http://arxiv.org/abs/2108.02922</a></li>
  <li>Rogers, A. (2020). Changing the World by Changing the Data. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2105.13947" target="_blank" rel="noopener">https://arxiv.org/abs/2105.13947</a></li>
  <li>Rolf, E., Worledge, T., Recht, B., &amp; Jordan, M. I. (2021). Representation Matters: Assessing the Importance of Subgroup Allocations in Training Data. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2103.03399" target="_blank"
      rel="noopener">https://arxiv.org/abs/2103.03399</a></li>
  <li>Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., &amp; Vertesi, J. (2019). Fairness and Abstraction in Sociotechnical Systems.<em> Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 59&ndash;68.
    <a href="https://doi.org/10.1145/3287560.3287598" target="_blank" rel="noopener">https://doi.org/10.1145/3287560.3287598</a></li>
  <li>Suresh, H., Movva, R., Lee Dogan, A., Bhargava, D., Isadora, C., Martinez Cuba, A., Taurino, G., So, W., & D’Ignazio, C. (2022). Towards Intersectional Feminist and Participatory ML: A Case Study in Supporting Femicide Counterdata Collection. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 667-678. <a
      href="https://doi.org/10.1145/3531146.3533132" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533132</a></li>
  <li>Stasaski, K., Yang, G. H., &amp; Hearst, M. A. (2020). More Diverse Dialogue Datasets via Diversity-Informed Data Collection. <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 4958&ndash;4968. <a
      href="https://doi.org/10.18653/v1/2020.acl-main.446" target="_blank" rel="noopener">https://doi.org/10.18653/v1/2020.acl-main.446</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>b. Creating New Datasets and/or Remediation of Existing Datasets</strong></h3>
<p class="fs-5 mb-4">This subsection includes articles that either remediate specific existing datasets or detail the creation of alternative datasets to address identified privacy and bias issues.</p>
<ul>
  <li>Asano, Y., Rupprecht, C., Zisserman, A., &amp; Vedaldi, A. (2021). PASS: An ImageNet Replacement for Self-Supervised Pretraining Without Humans. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2109.13228" target="_blank"
      rel="noopener">https://arxiv.org/abs/2109.13228</a></li>
  <li>Brown, H., Lee, K., Mireshghallah, F., Shokri, R., & Tramèr, F. (2022). What Does it Mean for a Language Model to Preserve Privacy? <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2280–2292. <a href="https://doi.org/10.1145/3531146.3534642" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3534642</a></li>
  <li>Cai, W., Encarnacion, R., Chern, B., Corbett-Davies, S., Bogen, M., Bergman, S., & Goel, S. (2022). Adaptive Sampling Strategies to Construct Equitable Training Datasets. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1467–1478. <a href="https://doi.org/10.1145/3531146.3533203" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533203</a></li>
  <li>Jernite, Y., Nguyen, H., Biderman, S., Rogers, A., Masoud, M., Danchev, V., Tan, S., Luccioni, A. S., Subramani, N., Johnson, I., Dupont, G., Dodge, J., Lo, K., Talat, Z., Radev, D., Gokaslan, A., Nikpoor, S., Henderson, P., Bommasani, R., & Mitchell, M. (2022). Data Governance in the Age of Large-Scale Data-Driven Language Technology. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2206–2222. <a href="https://doi.org/10.1145/3531146.3534637" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3534637</a></li>
  <li>Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., &amp; Roth, D. (2018). Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. <em>Proceedings of the 2018 Conference of the North American Chapter of
      the Association for Computational Linguistics: Human Language Technologies, 1</em>, 252&ndash;262. <a href="https://doi.org/10.18653/v1/N18-1023" target="_blank" rel="noopener">https://doi.org/10.18653/v1/N18-1023</a></li>
  <li>Yang, K., Qinami, K., Fei-Fei, L., Deng, J., &amp; Russakovsky, O. (2020). Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the ImageNet Hierarchy. <em>FAT* '20: Proceedings of the 2020 Conference on
      Fairness, Accountability, and Transparency</em>, 547-558. <a href="https://doi.org/10.1145/3351095.3375709" target="_blank" rel="noopener">https://doi.org/10.1145/3351095.3375709</a></li>
  <li>Yang, K., Yau, J., Fei-Fei, L., Deng, J., &amp; Russakovsky, O. (2021). A Study of Face Obfuscation in ImageNet. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2103.06191" target="_blank" rel="noopener">https://arxiv.org/abs/2103.06191</a></li>
  <li>Zellers, R., Bisk, Y., Schwartz, R., &amp; Choi, Y. (2018). SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. <em>ArXiv</em>. <a href="https://arxiv.org/abs/1808.05326v1" target="_blank"
      rel="noopener">https://arxiv.org/abs/1808.05326v1</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>c. Data Annotation Workflows</strong></h3>
<p class="fs-5 mb-4">Articles in this subsection address biased machine learning datasets by proposing changes to data annotation processes.</p>
<ul>
  <li>Barbosa, N. M., &amp; Chen, M. (2019). Rehumanized Crowdsourcing: A Labeling Framework Addressing Bias and Ethics in Machine Learning. <em>Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>, 1&ndash;12. <a
      href="http://doi.org/10.1145/3290605.3300773" target="_blank" rel="noopener">http://doi.org/10.1145/3290605.3300773</a></li>
  <li>Beretta, E., Vetr&ograve;, A., Lepri, B., &amp; Martin, J. C. D. (2021). Detecting Discriminatory Risk Through Data Annotation Based on Bayesian Inferences. <em>FAccT</em>. <a href="https://doi.org/10.1145/3442188.3445940" target="_blank"
      rel="noopener">https://doi.org/10.1145/3442188.3445940</a></li>
  <li>Beretta, E., Vetr&ograve;, A., Lepri, B., &amp; De Martin, J. C. (2019). Ethical and Socially-Aware Data Labels. In J. A. Lossio-Ventura, D. Mu&ntilde;ante, &amp; H. Alatrista-Salas (Eds.), <em>Information Management and Big Data</em>,
    320&ndash;327. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-11680-4_30" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-030-11680-4_30</a></li>
  <li>Rateike, M., Majumdar, A., Mineeva, O., Gummadi, K. P., & Valera, I. (2022). Don’t Throw it Away! The Utility of Unlabeled Data in Fair Decision Making. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1421–1433. <a href="https://doi.org/10.1145/3531146.3533199" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533199</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>d. Data Augmentation</strong></h3>
<p class="fs-5 mb-4">Articles in this subsection offer approaches to reducing bias in datasets by changing their composition via techniques such as oversampling or the use of synthetic/pseudo-data.</p>
<ul>
  <li>Iosifidis, V., &amp; Ntoutsi, E. (2018). Dealing with Bias via Data Augmentation in Supervised Learning Scenarios. <a href="http://ceur-ws.org/Vol-2103/paper_5.pdf" target="_blank" rel="noopener">http://ceur-ws.org/Vol-2103/paper_5.pdf</a></li>
  <li>Pastaltzidis, I., Dimitriou, N., Quezada-Tavarez, K., Aidinlis, S., Marquenie, T., Gurzawska, A., & Tzovaras, D. (2022). Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2302–2314. <a href="https://doi.org/10.1145/3531146.3534644" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3534644</a></li>
  <li>Sharma, S., Zhang, Y., R&iacute;os Aliaga, J. M., Bouneffouf, D., Muthusamy, V., &amp; Varshney, K. R. (2020). Data Augmentation for Discrimination Prevention and Bias Disambiguation. <em>Proceedings of the AAAI/ACM Conference on AI, Ethics,
      and Society</em>, 358&ndash;364. <a href="https://doi.org/10.1145/3375627.3375865" target="_blank" rel="noopener">https://doi.org/10.1145/3375627.3375865</a></li>
  <li>Tomalin, M., Byrne, B., Concannon, S., Saunders, D., &amp; Ullmann, S. (2021). The Practical Ethics of Bias Reduction in Machine Translation: Why Domain Adaptation is Better than Data Debiasing. <em>Ethics and Information Technology, 23</em>,
    419-433. <a href="https://doi.org/10.1007/s10676-021-09583-1" target="_blank" rel="noopener">https://doi.org/10.1007/s10676-021-09583-1</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>e. Bias Detection</strong></h3>
<p class="fs-5 mb-4">This subsection gathers tools and approaches for detecting bias in datasets.</p>
<ul>
  <li>Chapman, A., Grylls, P., Ugwudike, P., Gammack, D., & Ayling, J. (2022). A Data-Driven Analysis of the Interplay Between Criminology Theory and Predictive Policing Algorithms. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 36-45. <a href="https://doi.org/10.1145/3531146.3533071" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533071</a></li>
  <li>Goyal, P., Romero Soriano, A., Hazirbas, C., Levent, S., & Usunier, N. (2022). Fairness Indicators for Systematic Assessments of Visual Feature Extractors. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 70-88. <a href="https://doi.org/10.1145/3531146.3533074" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533074</a></li>
  <li>Harris, C., Halevy, M., Howard, A., Bruckman, A., & Yang, D. (2022). Exploring the Role of Grammar and Word Choice in Bias Toward African American English (AAE) in Hate Speech Classification. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 789-798. <a href="https://doi.org/10.1145/3531146.3533144" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533144</a></li>
  <li>Hu, X., Wang, H., Vegesana, A., Dube, S., Yu, K., Kao, G., Chen, S.-H., Lu, Y.-H., Thiruvathukal, G. K., &amp; Yin, M. (2020). Crowdsourcing Detection of Sampling Biases in Image Datasets. <em>Proceedings of The Web Conference 2020</em>,
    2955&ndash;2961. <a href="https://doi.org/10.1145/3366423.3380063" target="_blank" rel="noopener">https://doi.org/10.1145/3366423.3380063</a></li>
  <li>Leavy, S., Meaney, G., Wade, K., &amp; Greene, D. (2020). Mitigating Gender Bias in Machine Learning Data Sets. In L. Boratto, S. Faralli, M. Marras, &amp; G. Stilo (Eds.), <em>Bias and Social Aspects in Search and Recommendation</em>,
    12&ndash;26. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-52485-2_2" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-030-52485-2_2</a></li>
  <li>Pahl, J., Rieger, I., Mӧller, A., Wittenberg, T., & Schmid, U. (2022). Female, White, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 973-987. <a href="https://doi.org/10.1145/3531146.3533159" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533159</a></li>
  <li>Srinivasan, R., &amp; Chander, A. (n.d.). Understanding Bias in Datasets using Topological Data Analysis. 7. <a href="http://ceur-ws.org/Vol-2419/paper_9.pdf" target="_blank" rel="noopener">http://ceur-ws.org/Vol-2419/paper_9.pdf</a></li>
  <li>Verma, S., Ernst, M., &amp; Just, R. (2021). Removing Biased Data to Improve Fairness and Accuracy. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2102.03054" target="_blank" rel="noopener">https://arxiv.org/abs/2102.03054</a></li>
  <li>Wang, A., Barocas, S., Laird, K., & Wallach, H. (2022). Measuring Representational Harms in Image Captioning. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 324-335. <a href="https://doi.org/10.1145/3531146.3533099" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533099</a></li>
  <li>Wang, A., Narayanan, A., &amp; Russakovsky, O. (2020). REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets. <em>ECCV</em>, 733-751. <a href="https://doi.org/10.1007/978-3-030-58580-8_43" target="_blank"
      rel="noopener">https://doi.org/10.1007/978-3-030-58580-8_43</a></li>
  <li>Wang, A., Ramaswamy, V. V., & Russakovsky, O. (2022). Towards Intersectionality in Machine Learning: Including More Identities, Handling Underrepresetation, and Performing Evaluation. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 336-349. <a href="https://doi.org/10.1145/3531146.3533101" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533101</a></li>
  <li>Zamfirescu-Pereira, J. D., Chen, J., Wen, E, Koenecke, A., Garg, N., & Pierson, E. (2022) Trucks Don’t Mean Trump: Diagnosing Human Error in Image Analysis. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 799-813. <a href="https://doi.org/10.1145/3531146.3533145" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533145</a></li>
</ul>
<h3 class="fs-5 mb-4"><strong>f. Algorithms to Debias Datasets or Mitigate Bias</strong></h3>
<p class="fs-5 mb-4">Research in this subsection deploys algorithmic techniques to either debias datasets before training ML models on them or intervene to mitigate bias after training.</p>
<ul>
  <li>Abbasi-Sureshjani, S., Raumanns, R., Michels, B. E. J., Schouten, G., &amp; Cheplygina, V. (2020). Risk of Training Diagnostic Algorithms on Data with Demographic Bias. In J. Cardoso et al (Eds.), <em>Interpretable and Annotation-Efficient
      Learning for Medical Image Computing</em>, 183&ndash;192. Springer. <a href="https://doi.org/10.1007/978-3-030-61166-8_20" target="_blank" rel="noopener">https://doi.org/10.1007/978-3-030-61166-8_20</a></li>
  <li>Almuzaini, A. A., Bhatt, C. A., Pennock, D. M., & Singh, V. K. (2022). ABCinML: Anticipatory Bias Correction in Machine Learning Applications. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1552–1560. <a href="https://doi.org/10.1145/3531146.3533211" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533211</a></li>
  <li>Anahideh, H., Asudeh, A., &amp; Thirumuruganathan, S. (2021). Fair Active Learning. <em>ArXiv</em>. <a href="http://arxiv.org/abs/2001.0179" target="_blank" rel="noopener">http://arxiv.org/abs/2001.0179</a></li>
  <li>Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., &amp; Kalai, A. (2016). Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1607.06520" target="_blank"
      rel="noopener">http://arxiv.org/abs/1607.06520</a></li>
  <li>Hendricks, L. A., Burns, K., Saenko, K., Darrell, T., &amp; Rohrbach, A. (2018). Women Also Snowboard: Overcoming Bias in Captioning Models. <em>ECCV</em>, 771&ndash;787. <a
      href="https://openaccess.thecvf.com/content_ECCV_2018/html/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.html" target="_blank"
      rel="noopener">https://openaccess.thecvf.com/content_ECCV_2018/html/Lisa_Anne_Hendricks_Women_also_Snowboard_ECCV_2018_paper.html</a></li>
  <li>Lum, K., Zhang, Y., & Bower, A. (2022). De-Biasing “Bias” Measurement. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 379-389. <a href="https://doi.org/10.1145/3531146.3533105" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533105</a></li>
  <li>Reimers, C., Bodesheim, P., Runge, J., &amp; Denzler, J. (2021). Towards Learning an Unbiased Classifier from Biased Data via Conditional Adversarial Debiasing. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2103.06179" target="_blank"
      rel="noopener">https://arxiv.org/abs/2103.06179</a></li>
  <li>Ryu, H. J., Mitchell, M., &amp; Adam, H. (2017). InclusiveFaceNet: Improving Face Attribute Detection with Race and Gender Diversity. <em>ArXiv</em>. <a href="https://arxiv.org/abs/1712.00193" target="_blank"
      rel="noopener">https://arxiv.org/abs/1712.00193</a></li>
  <li>Schick, T., Udupa, S., &amp; Sch&uuml;tze, H. (2021). Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP.<em> ArXiv</em>. <a href="https://arxiv.org/abs/2103.00453" target="_blank"
      rel="noopener">https://arxiv.org/abs/2103.00453</a></li>
  <li>Sikdar, S., Lemmerich, F., & Strohmaier, M. (2022). GetFair: Generalized Fairness Tuning of Classification Models. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 289-299. <a href="https://doi.org/10.1145/3531146.3533094" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533094</a></li>
  <li>Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp; Chang, K.-W. (2017). Men Also Like Shopping: Reducing Gender Bias Amplification Using Corpus-level Constraints. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1707.09457" target="_blank"
      rel="noopener">http://arxiv.org/abs/1707.09457</a></li>
</ul>



<h2 class="fs-5 mb-4" id="dataset_documentation_practices"><strong>7. DATASET DOCUMENTATION PRACTICES&nbsp;</strong></h2>
<p class="fs-5 mb-4">In recent years, there have been calls to increase transparency and standardization for ML datasets so that researchers can better study their composition and effects, as well as identify problems. This section collects these
  various approaches to dataset documentation.</p>
<ul>
  <li>Bandy, J., &amp; Vincent, N. (2021). Addressing &ldquo;Documentation Debt&rdquo; in Machine Learning Research: A Retrospective Datasheet for BookCorpus. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2105.05241" target="_blank"
      rel="noopener">https://arxiv.org/abs/2105.05241</a></li>
  <li>Barclay, I., Preece, A., Taylor, I., Radha, S. K., &amp; Nabrzyski, J. (2021). Providing Assurance and Scrutability on Shared Data and Machine Learning Models with Verifiable Credentials. <em>ArXiv</em>. <a
      href="http://arxiv.org/abs/2105.06370" target="_blank" rel="noopener">http://arxiv.org/abs/2105.06370</a></li>
  <li>​​Barclay, I., Preece, A., Taylor, I., &amp; Verma, D. (2019). Towards Traceability in Data Ecosystems Using a Bill of Materials Model. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1904.04253" target="_blank"
      rel="noopener">http://arxiv.org/abs/1904.04253</a></li>
  <li>Bender, E. M., &amp; Friedman, B. (2018). Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. <em>Transactions of the Association for Computational Linguistics</em>, <em>6</em>,
    587&ndash;604. <a href="https://doi.org/10.1162/tacl_a_00041" target="_blank" rel="noopener">https://doi.org/10.1162/tacl_a_00041</a></li>
  <li>Benjamin, M., Gagnon, P., Rostamzadeh, N., Pal, C., Bengio, Y., &amp; Shee, A. (2019). Towards Standardization of Data Licenses: The Montreal Data License. <em>ArXiv. </em> <a href="https://doi.org/10.48550/arXiv.1903.12262" target="_blank" rel="noopener">https://doi.org/10.48550/arXiv.1903.12262 </a></li>
  <li>Boyd, K. (2020). Understanding and Intervening in Machine Learning Ethics: Supporting Ethical Sensitivity in Training Data Curation. ProQuest [University of Maryland, College Park]. <a
      href="https://www.proquest.com/openview/046800aae7b57cc51efdc1caa7a84cba/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y" target="_blank"
      rel="noopener">https://www.proquest.com/openview/046800aae7b57cc51efdc1caa7a84cba/1?pq-origsite=gscholar&amp;cbl=18750&amp;diss=y</a></li>
  <li>Crisan, A., Drouhard, M., Vig, J., & Rajani, N. (2022). Interactive Model Cards: A Human-Centered Approach to Model Documentation. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 427-439. <a href="https://doi.org/10.1145/3531146.3533108" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533108</a></li>
  <li>Díaz, M., Kivlichan, I., Rosen, R., Baker, D., Amironesei, R., Prabhakaran, V., & Denton, E. (2022). CrowdWorkSheets: Accounting for Individual and Collective Identities Underlying Crowdsourced Dataset Annotation. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 2342–2351. <a href="https://doi.org/10.1145/3531146.3534647" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3534647</a></li>
  <li>Fabris, A., Messina, S., Silvello, G., &amp; Susto, G. A. (2022). Algorithmic Fairness Datasets: The Story so Far. <em>ArXiv</em>. <a href="https://doi.org/10.48550/arXiv.2202.01711" target="_blank" rel="noopener">https://doi.org/10.48550/arXiv.2202.01711</a></li>
  <li>Gansky, B., & McDonald, S. (2022). CounterFAccTual: How FAccT Undermines Its Organizing Principles. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1982–1992. <a href="https://doi.org/10.1145/3531146.3533241" target="_blank"
      rel="noopener">https://doi.org/10.1145/3531146.3533241</a></li>
  <li>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daum&eacute; III, H., &amp; Crawford, K. (2020). Datasheets for Datasets. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1803.09010" target="_blank"
      rel="noopener">http://arxiv.org/abs/1803.09010</a></li>
  <li>Holland, S., Hosny, A., Newman, S., Joseph, J., &amp; Chmielinski, K. (2018). The Dataset Nutrition Label: A Framework To Drive Higher Data Quality Standards. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1805.03677" target="_blank"
      rel="noopener">http://arxiv.org/abs/1805.03677</a></li>
  <li>Luccioni, A. S., Corry, F., Sridharan, H., Ananny, M., Schultz, J., & Crawford, K. (2022). A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 199–212. <a href="https://doi.org/10.1145/3531146.3533086" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533086</a></li>
  <li>McMillan-Major, A., Osei, S., Rodriguez, J. D., Ammanamanchi, P. S., Gehrmann, S., &amp; Jernite, Y. (2021). Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the
    HuggingFace and GEM Data and Model Cards. <em>ArXiv</em>. <a href="https://arxiv.org/abs/2108.07374" target="_blank" rel="noopener">https://arxiv.org/abs/2108.07374</a></li>
  <li>Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D., &amp; Gebru, T. (2019). Model Cards for Model Reporting. <em>Proceedings of the Conference on Fairness, Accountability, and
      Transparency</em>, 220&ndash;229. <a href="https://doi.org/10.1145/3287560.3287596" target="_blank" rel="noopener">https://doi.org/10.1145/3287560.3287596</a></li>
  <li>Pushkarna, M., Zaldivar, A., & Kjartansson, O. (2022). Data Cards: Purposeful and Transparent Dataset Documentation for Responsible AI. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1776–1826. <a href="https://doi.org/10.1145/3531146.3533231" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533231</a></li>
  <li>Rostamzadeh, N., Mincu, D., Roy, S., Smart, A., Wilcox, L., Pushkarna, M., Schrouff, J., Amironesei, R., Moorosi, N., & Heller, K. (2022). Healthsheet: Development of a Transparency Artifact for Health Datasets. <i>2022 ACM Conference on Fairness, Accountability, and Transparency,</i>, 1943–1961. <a href="https://doi.org/10.1145/3531146.3533239" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533239</a></li>
  <li>Seck, I., Dahmane, K., Duthon, P., &amp; Loosli, G. (2018). Baselines and a Datasheet for the Cerema AWP dataset. <em>ArXiv</em>. <a href="http://arxiv.org/abs/1806.04016" target="_blank" rel="noopener">http://arxiv.org/abs/1806.04016</a></li>
  <li>Schramowski, P., Tauchmann, C., & Kersting, K. (2022). Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content? <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 1350–1361. <a href="https://doi.org/10.1145/3531146.3533192" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533192</a></li>
  <li>Srinivasan, R., Denton, E., Famularo, J., Rostamzadeh, N., Diaz, F., &amp; Coleman, B. (2021). Artsheets for Art Datasets. <em>Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>. <a href="https://openreview.net/forum?id=K7ke_GZ_6N" target="_blank" rel="noopener">https://openreview.net/forum?id=K7ke_GZ_6N </a></li>
  <li>Zhang, W., Ohrimenko, O., & Cummings, R. (2022). Attribute Privacy: Framework and Mechanisms. <i>2022 ACM Conference on Fairness, Accountability, and Transparency</i>, 757-766. <a href="https://doi.org/10.1145/3531146.3533139" target="_blank" rel="noopener">https://doi.org/10.1145/3531146.3533139</a></li>
</ul>


<h2 class="fs-5 mb-4" id="conferences_focused_on_datasets"><strong>8. CONFERENCES FOCUSED ON DATASETS&nbsp;</strong></h2>
<p class="fs-5 mb-4">The scholarship summarized in this list spans academic fields, from science and technology studies (STS) to computer science, and human computer interaction (HCI) to library science. During the construction of this list, it
  became clear that certain conference venues and their proceedings are often associated with emerging work on training data. Other conference venues have dedicated workshops or particular tracks to the study of datasets. While this broader list
  represents training data scholarship at a particular moment in time, these locales provide sites where work on training data has been concentrated or is likely to be found.</p>
<ul>
  <li>ACM CHI Conference on Human Factors in Computing Systems <a href="https://chi2021.acm.org/" target="_blank" rel="noopener">https://chi2021.acm.org/</a></li>
  <li>ACM Conference on Computer Supported Cooperative Work (CSCW) <a href="https://dl.acm.org/conference/cscw" target="_blank" rel="noopener">https://dl.acm.org/conference/cscw</a></li>
  <li>ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) <a href="https://facctconference.org/" target="_blank" rel="noopener">https://facctconference.org/</a></li>
  <li>NeurIPS Datasets and Benchmarks Track <a href="https://neurips.cc/Conferences/2021/CallForDatasetsBenchmarks" target="_blank" rel="noopener">https://neurips.cc/Conferences/2021/CallForDatasetsBenchmarks</a></li>
  <li>NeurIPS Data-Centric AI Workshop (2021) <a href="https://nips.cc/Conferences/2021/Schedule?showEvent=21860" target="_blank" rel="noopener">https://nips.cc/Conferences/2021/Schedule?showEvent=21860</a></li>
</ul>


<h2 class="fs-5 mb-4" id="press_treatment_of_datasets"><strong>9. PRESS TREATMENT OF DATASETS</strong></h2>
<p class="fs-5 mb-4">Popular press treatments of training data have provided a foundation for broader public conversations about these artifacts. The press gathered here represents just a small sample of both the important investigative work into
  training data as well as cogent introductions to the subject. Articles are frequently published on these issues, so this is just a selection of starting points.&nbsp;</p>
<ul>
  <li>Argoub, S. (2021, June 9). The NLP Divide: English is Not the Only Natural Language. <em>Polis</em>. <a href="https://blogs.lse.ac.uk/polis/2021/06/09/the-nlp-divide-english-is-not-the-only-natural-language/" target="_blank"
      rel="noopener">https://blogs.lse.ac.uk/polis/2021/06/09/the-nlp-divide-english-is-not-the-only-natural-language/</a></li>
  <li>Buranyi, S. (2017, August 8). Rise of the Racist Robots &ndash; How AI is Learning All Our Worst Impulses.<em> The Guardian</em>. <a
      href="http://www.theguardian.com/inequality/2017/aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses" target="_blank" rel="noopener">
      http://www.theguardian.com/inequality/2017/aug/08/rise-of-the-racist-robots-how-ai-is-learning-all-our-worst-impulses</a></li>
  <li>Elliott, V. (2021, August 3). Training Self-Driving Cars for $1 an Hour. <em>Rest of World</em>. <a href="https://restofworld.org/2021/self-driving-cars-outsourcing/" target="_blank" rel="noopener">
  https://restofworld.org/2021/self-driving-cars-outsourcing/</a> </li>
  <li>McQuaid, J. (2021, October 18). Can AI&rsquo;s Voracious Appetite Be Tamed? <em>Undark Magazine</em>. <a href="https://undark.org/2021/10/18/computer-scientists-try-to-sidestep-ai-data-dilemma/" target="_blank"
      rel="noopener">https://undark.org/2021/10/18/computer-scientists-try-to-sidestep-ai-data-dilemma/</a></li>
  <li>Smith, C. S. (2019, November 19). Dealing With Bias in Artificial Intelligence. <em>The New York Times</em>. <a href="https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-bias.html" target="_blank"
      rel="noopener">https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-bias.html</a></li>
  <li>Feathers, T. (2020, September 17). Fake Data Could Help Solve Machine Learning&rsquo;s Bias Problem&mdash;If We Let It. <em>Slate Magazine</em>. <a href="https://slate.com/technology/2020/09/synthetic-data-artificial-intelligence-bias.html"
      target="_blank" rel="noopener">https://slate.com/technology/2020/09/synthetic-data-artificial-intelligence-bias.html</a></li>
  <li>Gershgorn, D. (2018, September 6). If AI is Going to Be the World&rsquo;s Doctor, It Needs Better Textbooks. <em>Quartz</em>. <a href="https://qz.com/1367177/if-ai-is-going-to-be-the-worlds-doctor-it-needs-better-textbooks/" target="_blank"
      rel="noopener">https://qz.com/1367177/if-ai-is-going-to-be-the-worlds-doctor-it-needs-better-textbooks/</a></li>
  <li>Johnson, K. (2021, June 17). The Efforts to Make Text-Based AI Less Racist and Terrible. <em>Wired</em>. <a href="https://www.wired.com/story/efforts-make-text-ai-less-racist-terrible/" target="_blank"
      rel="noopener">https://www.wired.com/story/efforts-make-text-ai-less-racist-terrible/</a></li>
  <li><span style="color: #222222;">Johnson, K. (2021, August 4). This New Way to Train AI Could Curb Online Harassment.</span><span style="color: #222222;"><em> Wired</em></span><span style="color: #222222;">. </span><a
      href="https://www.wired.com/story/new-way-train-ai-curb-online-harassment/" target="_blank" rel="noopener">https://www.wired.com/story/new-way-train-ai-curb-online-harassment/</a></li>
  <li>Metz, C. (2019, September 20). &lsquo;Nerd&rsquo;, &lsquo;Nonsmoker, &lsquo;Wrongdoer,&rsquo;: How Might A.I. Label You?, <em>The New York Times</em>. <span
      style="text-decoration: underline;"> <a href="https://www.nytimes.com/2019/09/20/arts/design/imagenet-trevor-paglen-ai-facial-recognition.html" target="_blank"
          rel="noopener">https://www.nytimes.com/2019/09/20/arts/design/imagenet-trevor-paglen-ai-facial-recognition.html</a></span></li>
  <li>Murgia, M., &amp; Harlow, M. (2019, April 19). Who&rsquo;s Using Your face? The Ugly Truth About Facial Recognition. <em>Financial Times</em>. <a href="https://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e" target="_blank"
      rel="noopener">https://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e</a></li>
  <li>Register, Y. L.. (2021, July 22). It&rsquo;s All Training Data: Using Lessons from Machine Learning to Retrain Your Mind. <em>The Gradient</em>. <a href="https://thegradient.pub/its-all-training-data/" target="_blank"
      rel="noopener">https://thegradient.pub/its-all-training-data/</a></li>
  <li>Solon, O. (2019, March 12). Facial Recognition&rsquo;s &ldquo;Dirty Little Secret&rdquo;: Social Media Photos Used Without Consent. <em>NBC News. </em><a
      href="https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921" target="_blank"
      rel="noopener">https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921</a></li>
  <li>Solon, O., &amp; Farivar, C. (2019, May 9). Millions of People Uploaded Photos to the Ever App. Then the Company Used Them to Develop Facial Recognition Tools. <em>NBC News. </em><a
      href="https://www.nbcnews.com/tech/security/millions-people-uploaded-photos-ever-app-then-company-used-them-n1003371" target="_blank"
      rel="noopener">https://www.nbcnews.com/tech/security/millions-people-uploaded-photos-ever-app-then-company-used-them-n1003371</a></li>
</ul>

</section>
</article>
<!-- Comments section-->

</div>
</div>
</div>
</section>
</main>

<!-- Form  -->
<div id="submitform">
<iframe src="https://docs.google.com/forms/d/e/1FAIpQLScmvcD0naOd-szpeldPMTc0A7nwWClaNATS3VsXlm6J1Xo_tw/viewform?embedded=true" width="640" height="975" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>
</div>
        <!-- Footer-->
        <footer class="bg-dark py-4 mt-auto">
            <div class="container px-5">
                <div class="row align-items-center justify-content-between flex-column flex-sm-row">
                    <div class="col-auto"><div class="small m-0 text-white">Copyright &copy; Knowing Machines 2022 - CC-BY 4.0 International</div></div>
                    <div class="col-auto">

                    </div>
                </div>
            </div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
